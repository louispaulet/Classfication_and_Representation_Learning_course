{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TUTORIAL 3 MNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO5nZ0bKKJn3M7ATMrz4xB5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louispaulet/Classfication_and_Representation_Learning_course/blob/main/TUTORIAL_3_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on6Co97FCxi9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0999f3b2-4ecb-4fe4-bde7-65b81642fd9e"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-78fcb653-ed67-822c-1198-ce1c74be42bf)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEOPnO5fDmaQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "ee871b17-a0f5-43a4-9834-fe3608d358ba"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Oct 26 14:02:31 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCBClw14JpkR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "fb1a73fb-c082-41ad-f6b9-24b7038091e8"
      },
      "source": [
        "!pip install --upgrade https://github.com/Theano/Theano/archive/master.zip\n",
        "!pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/Theano/Theano/archive/master.zip\n",
            "  Using cached https://github.com/Theano/Theano/archive/master.zip\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Theano==1.0.5+unknown) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Theano==1.0.5+unknown) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Theano==1.0.5+unknown) (1.15.0)\n",
            "Building wheels for collected packages: Theano\n",
            "  Building wheel for Theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Theano: filename=Theano-1.0.5+unknown-cp36-none-any.whl size=2667284 sha256=cb17d59ad7085510c0cc7de2c24e1fd55fd5e8c8c349abc205cdeffbad0a940b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_npzvsgp/wheels/33/73/96/0ed263c62a86e2485ea634e0d3ae8169d50fd66e3b252541db\n",
            "Successfully built Theano\n",
            "Installing collected packages: Theano\n",
            "  Found existing installation: Theano 1.0.5+unknown\n",
            "    Uninstalling Theano-1.0.5+unknown:\n",
            "      Successfully uninstalled Theano-1.0.5+unknown\n",
            "Successfully installed Theano-1.0.5+unknown\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "theano"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/Lasagne/Lasagne/archive/master.zip\n",
            "  Using cached https://github.com/Lasagne/Lasagne/archive/master.zip\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from Lasagne==0.2.dev1) (1.18.5)\n",
            "Building wheels for collected packages: Lasagne\n",
            "  Building wheel for Lasagne (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Lasagne: filename=Lasagne-0.2.dev1-cp36-none-any.whl size=122797 sha256=c50c878e4ec60018c73152ddb2e31a3bde5e2ba447921efc0637cfc5a9b34922\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5z1wrrbr/wheels/ca/4a/00/87f1777b229481fe76562df7c0cfb993bc88ed0cc37e3f0ed4\n",
            "Successfully built Lasagne\n",
            "Installing collected packages: Lasagne\n",
            "  Found existing installation: Lasagne 0.2.dev1\n",
            "    Uninstalling Lasagne-0.2.dev1:\n",
            "      Successfully uninstalled Lasagne-0.2.dev1\n",
            "Successfully installed Lasagne-0.2.dev1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "lasagne"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCl-fyftJbj0"
      },
      "source": [
        "import lasagne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxSnuoVyJ7lU"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import theano\n",
        "import theano.tensor as T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJa1NvjKLq5t"
      },
      "source": [
        "# ################## Download and prepare the MNIST dataset ##################\n",
        "# This is just some way of getting the MNIST dataset from an online location\n",
        "# and loading it into numpy arrays. It doesn't involve Lasagne at all.\n",
        "\n",
        "def load_dataset():\n",
        "    # We first define a download function, supporting both Python 2 and 3.\n",
        "    if sys.version_info[0] == 2:\n",
        "        from urllib import urlretrieve\n",
        "    else:\n",
        "        from urllib.request import urlretrieve\n",
        "\n",
        "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
        "        print(\"Downloading %s\" % filename)\n",
        "        urlretrieve(source + filename, filename)\n",
        "\n",
        "    # We then define functions for loading MNIST images and labels.\n",
        "    # For convenience, they also download the requested files if needed.\n",
        "    import gzip\n",
        "\n",
        "    def load_mnist_images(filename):\n",
        "        if not os.path.exists(filename):\n",
        "            download(filename)\n",
        "        # Read the inputs in Yann LeCun's binary format.\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
        "        # following the shape convention: (examples, channels, rows, columns)\n",
        "        data = data.reshape(-1, 1, 28, 28)\n",
        "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
        "        # (Actually to range [0, 255/256], for compatibility to the version\n",
        "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
        "        return data / np.float32(256)\n",
        "\n",
        "    def load_mnist_labels(filename):\n",
        "        if not os.path.exists(filename):\n",
        "            download(filename)\n",
        "        # Read the labels in Yann LeCun's binary format.\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "        # The labels are vectors of integers now, that's exactly what we want.\n",
        "        return data\n",
        "\n",
        "    # We can now download and read the training and test set images and labels.\n",
        "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
        "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
        "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
        "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "    # We reserve the last 10000 training examples for validation.\n",
        "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
        "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "\n",
        "    # We just return all the arrays in order, as expected in main().\n",
        "    # (It doesn't matter how we do this as long as we can read them again.)\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS2fKIC8L4p4"
      },
      "source": [
        "# ##################### Build the neural network model #######################\n",
        "# This script supports three types of models. For each one, we define a\n",
        "# function that takes a Theano variable representing the input and returns\n",
        "# the output layer of a neural network model built in Lasagne.\n",
        "\n",
        "def build_mlp(input_var=None):\n",
        "    # This creates an MLP of two hidden layers of 800 units each, followed by\n",
        "    # a softmax output layer of 10 units. It applies 20% dropout to the input\n",
        "    # data and 50% dropout to the hidden layers.\n",
        "\n",
        "    # Input layer, specifying the expected input shape of the network\n",
        "    # (unspecified batchsize, 1 channel, 28 rows and 28 columns) and\n",
        "    # linking it to the given Theano variable `input_var`, if any:\n",
        "    l_in = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
        "                                     input_var=input_var)\n",
        "\n",
        "    # Apply 20% dropout to the input data:\n",
        "    l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
        "\n",
        "    # Add a fully-connected layer of 800 units, using the linear rectifier, and\n",
        "    # initializing weights with Glorot's scheme (which is the default anyway):\n",
        "    l_hid1 = lasagne.layers.DenseLayer(\n",
        "            l_in_drop, num_units=800,\n",
        "            nonlinearity=lasagne.nonlinearities.rectify,\n",
        "            W=lasagne.init.GlorotUniform())\n",
        "\n",
        "    # We'll now add dropout of 50%:\n",
        "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
        "\n",
        "    # Another 800-unit layer:\n",
        "    l_hid2 = lasagne.layers.DenseLayer(\n",
        "            l_hid1_drop, num_units=800,\n",
        "            nonlinearity=lasagne.nonlinearities.rectify)\n",
        "\n",
        "    # 50% dropout again:\n",
        "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.5)\n",
        "\n",
        "    # Finally, we'll add the fully-connected output layer, of 10 softmax units:\n",
        "    l_out = lasagne.layers.DenseLayer(\n",
        "            l_hid2_drop, num_units=10,\n",
        "            nonlinearity=lasagne.nonlinearities.softmax)\n",
        "\n",
        "    # Each layer is linked to its incoming layer(s), so we only need to pass\n",
        "    # the output layer to give access to a network in Lasagne:\n",
        "    return l_out\n",
        "\n",
        "\n",
        "def build_custom_mlp(input_var=None, depth=2, width=800, drop_input=.2,\n",
        "                     drop_hidden=.5):\n",
        "    # By default, this creates the same network as `build_mlp`, but it can be\n",
        "    # customized with respect to the number and size of hidden layers. This\n",
        "    # mostly showcases how creating a network in Python code can be a lot more\n",
        "    # flexible than a configuration file. Note that to make the code easier,\n",
        "    # all the layers are just called `network` -- there is no need to give them\n",
        "    # different names if all we return is the last one we created anyway; we\n",
        "    # just used different names above for clarity.\n",
        "\n",
        "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
        "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
        "                                        input_var=input_var)\n",
        "    if drop_input:\n",
        "        network = lasagne.layers.dropout(network, p=drop_input)\n",
        "    # Hidden layers and dropout:\n",
        "    nonlin = lasagne.nonlinearities.rectify\n",
        "    for _ in range(depth):\n",
        "        network = lasagne.layers.DenseLayer(\n",
        "                network, width, nonlinearity=nonlin)\n",
        "        if drop_hidden:\n",
        "            network = lasagne.layers.dropout(network, p=drop_hidden)\n",
        "    # Output layer:\n",
        "    softmax = lasagne.nonlinearities.softmax\n",
        "    network = lasagne.layers.DenseLayer(network, 10, nonlinearity=softmax)\n",
        "    return network\n",
        "\n",
        "\n",
        "def build_cnn(input_var=None):\n",
        "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
        "    # and a fully-connected hidden layer in front of the output layer.\n",
        "\n",
        "    # Input layer, as usual:\n",
        "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
        "                                        input_var=input_var)\n",
        "    # This time we do not apply input dropout, as it tends to work less well\n",
        "    # for convolutional layers.\n",
        "\n",
        "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
        "    # convolutions are supported as well; see the docstring.\n",
        "    network = lasagne.layers.Conv2DLayer(\n",
        "            network, num_filters=32, filter_size=(5, 5),\n",
        "            nonlinearity=lasagne.nonlinearities.rectify,\n",
        "            W=lasagne.init.GlorotUniform())\n",
        "    # Expert note: Lasagne provides alternative convolutional layers that\n",
        "    # override Theano's choice of which implementation to use; for details\n",
        "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
        "\n",
        "    # Max-pooling layer of factor 2 in both dimensions:\n",
        "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
        "\n",
        "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
        "    network = lasagne.layers.Conv2DLayer(\n",
        "            network, num_filters=32, filter_size=(5, 5),\n",
        "            nonlinearity=lasagne.nonlinearities.rectify)\n",
        "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
        "\n",
        "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
        "    network = lasagne.layers.DenseLayer(\n",
        "            lasagne.layers.dropout(network, p=.5),\n",
        "            num_units=256,\n",
        "            nonlinearity=lasagne.nonlinearities.rectify)\n",
        "\n",
        "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
        "    network = lasagne.layers.DenseLayer(\n",
        "            lasagne.layers.dropout(network, p=.5),\n",
        "            num_units=10,\n",
        "            nonlinearity=lasagne.nonlinearities.softmax)\n",
        "\n",
        "    return network\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz8XzGcIMFtK"
      },
      "source": [
        "# ############################# Batch iterator ###############################\n",
        "# This is just a simple helper function iterating over training data in\n",
        "# mini-batches of a particular size, optionally in random order. It assumes\n",
        "# data is available as numpy arrays. For big datasets, you could load numpy\n",
        "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
        "# own custom data iteration function. For small datasets, you can also copy\n",
        "# them to GPU at once for slightly improved performance. This would involve\n",
        "# several changes in the main program, though, and is not demonstrated here.\n",
        "# Notice that this function returns only mini-batches of size `batchsize`.\n",
        "# If the size of the data is not a multiple of `batchsize`, it will not\n",
        "# return the last (remaining) mini-batch.\n",
        "\n",
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(inputs))\n",
        "        np.random.shuffle(indices)\n",
        "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield inputs[excerpt], targets[excerpt]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la1U00aEMJdC"
      },
      "source": [
        "# ############################## Main program ################################\n",
        "# Everything else will be handled in our main program now. We could pull out\n",
        "# more functions to better separate the code, but it wouldn't make it any\n",
        "# easier to read.\n",
        "\n",
        "def main(model='mlp', num_epochs=500):\n",
        "    # Load the dataset\n",
        "    print(\"Loading data...\")\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
        "\n",
        "    # Prepare Theano variables for inputs and targets\n",
        "    input_var = T.tensor4('inputs')\n",
        "    target_var = T.ivector('targets')\n",
        "\n",
        "    # Create neural network model (depending on first command line parameter)\n",
        "    print(\"Building model and compiling functions...\")\n",
        "    if model == 'mlp':\n",
        "        network = build_mlp(input_var)\n",
        "    elif model.startswith('custom_mlp:'):\n",
        "        depth, width, drop_in, drop_hid = model.split(':', 1)[1].split(',')\n",
        "        network = build_custom_mlp(input_var, int(depth), int(width),\n",
        "                                   float(drop_in), float(drop_hid))\n",
        "    elif model == 'cnn':\n",
        "        network = build_cnn(input_var)\n",
        "    else:\n",
        "        print(\"Unrecognized model type %r.\" % model)\n",
        "        return\n",
        "\n",
        "    # Create a loss expression for training, i.e., a scalar objective we want\n",
        "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
        "    prediction = lasagne.layers.get_output(network)\n",
        "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
        "    loss = loss.mean()\n",
        "    # We could add some weight decay as well here, see lasagne.regularization.\n",
        "\n",
        "    # Create update expressions for training, i.e., how to modify the\n",
        "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
        "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
        "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
        "    updates = lasagne.updates.nesterov_momentum(\n",
        "            loss, params, learning_rate=0.05, momentum=0.9)\n",
        "\n",
        "    # Create a loss expression for validation/testing. The crucial difference\n",
        "    # here is that we do a deterministic forward pass through the network,\n",
        "    # disabling dropout layers.\n",
        "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
        "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
        "                                                            target_var)\n",
        "    test_loss = test_loss.mean()\n",
        "    # As a bonus, also create an expression for the classification accuracy:\n",
        "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
        "                      dtype=theano.config.floatX)\n",
        "\n",
        "    # Compile a function performing a training step on a mini-batch (by giving\n",
        "    # the updates dictionary) and returning the corresponding training loss:\n",
        "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
        "\n",
        "    # Compile a second function computing the validation loss and accuracy:\n",
        "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
        "\n",
        "    # Finally, launch the training loop.\n",
        "    print(\"Starting training...\")\n",
        "    # We iterate over epochs:\n",
        "    for epoch in range(num_epochs):\n",
        "        # In each epoch, we do a full pass over the training data:\n",
        "        train_err = 0\n",
        "        train_batches = 0\n",
        "        start_time = time.time()\n",
        "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
        "            inputs, targets = batch\n",
        "            train_err += train_fn(inputs, targets)\n",
        "            train_batches += 1\n",
        "\n",
        "        # And a full pass over the validation data:\n",
        "        val_err = 0\n",
        "        val_acc = 0\n",
        "        val_batches = 0\n",
        "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
        "            inputs, targets = batch\n",
        "            err, acc = val_fn(inputs, targets)\n",
        "            val_err += err\n",
        "            val_acc += acc\n",
        "            val_batches += 1\n",
        "\n",
        "        # Then we print the results for this epoch:\n",
        "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "            epoch + 1, num_epochs, time.time() - start_time))\n",
        "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
        "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
        "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
        "            val_acc / val_batches * 100))\n",
        "\n",
        "    # After training, we compute and print the test error:\n",
        "    test_err = 0\n",
        "    test_acc = 0\n",
        "    test_batches = 0\n",
        "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
        "        inputs, targets = batch\n",
        "        err, acc = val_fn(inputs, targets)\n",
        "        test_err += err\n",
        "        test_acc += acc\n",
        "        test_batches += 1\n",
        "    print(\"Final results:\")\n",
        "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
        "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
        "        test_acc / test_batches * 100))\n",
        "\n",
        "    # Optionally, you could now dump the network weights to a file like this:\n",
        "    np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
        "    print('saved the model')\n",
        "    #\n",
        "    # And load them again later on like this:\n",
        "    # with np.load('model.npz') as f:\n",
        "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
        "    # lasagne.layers.set_all_param_values(network, param_values)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     if ('--help' in sys.argv) or ('-h' in sys.argv):\n",
        "#         print(\"Trains a neural network on MNIST using Lasagne.\")\n",
        "#         print(\"Usage: %s [MODEL [EPOCHS]]\" % sys.argv[0])\n",
        "#         print()\n",
        "#         print(\"MODEL: 'mlp' for a simple Multi-Layer Perceptron (MLP),\")\n",
        "#         print(\"       'custom_mlp:DEPTH,WIDTH,DROP_IN,DROP_HID' for an MLP\")\n",
        "#         print(\"       with DEPTH hidden layers of WIDTH units, DROP_IN\")\n",
        "#         print(\"       input dropout and DROP_HID hidden dropout,\")\n",
        "#         print(\"       'cnn' for a simple Convolutional Neural Network (CNN).\")\n",
        "#         print(\"EPOCHS: number of training epochs to perform (default: 500)\")\n",
        "#     else:\n",
        "#         kwargs = {}\n",
        "#         if len(sys.argv) > 1:\n",
        "#             kwargs['model'] = sys.argv[1]\n",
        "#         if len(sys.argv) > 2:\n",
        "#             print(sys.argv[2])\n",
        "#             kwargs['num_epochs'] = int(sys.argv[2])\n",
        "#         main(**kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR8ZokrNNGXD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d32b067a-052a-45cf-f399-7f26f6985fcb"
      },
      "source": [
        "main(num_epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Building model and compiling functions...\n",
            "Starting training...\n",
            "Epoch 1 of 50 took 16.654s\n",
            "  training loss:\t\t0.716297\n",
            "  validation loss:\t\t0.232704\n",
            "  validation accuracy:\t\t93.02 %\n",
            "Epoch 2 of 50 took 16.296s\n",
            "  training loss:\t\t0.328577\n",
            "  validation loss:\t\t0.162775\n",
            "  validation accuracy:\t\t95.38 %\n",
            "Epoch 3 of 50 took 16.027s\n",
            "  training loss:\t\t0.258248\n",
            "  validation loss:\t\t0.130322\n",
            "  validation accuracy:\t\t96.25 %\n",
            "Epoch 4 of 50 took 16.005s\n",
            "  training loss:\t\t0.218992\n",
            "  validation loss:\t\t0.114461\n",
            "  validation accuracy:\t\t96.64 %\n",
            "Epoch 5 of 50 took 16.022s\n",
            "  training loss:\t\t0.190098\n",
            "  validation loss:\t\t0.100796\n",
            "  validation accuracy:\t\t97.05 %\n",
            "Epoch 6 of 50 took 16.407s\n",
            "  training loss:\t\t0.173764\n",
            "  validation loss:\t\t0.091978\n",
            "  validation accuracy:\t\t97.19 %\n",
            "Epoch 7 of 50 took 16.844s\n",
            "  training loss:\t\t0.158231\n",
            "  validation loss:\t\t0.087311\n",
            "  validation accuracy:\t\t97.34 %\n",
            "Epoch 8 of 50 took 16.267s\n",
            "  training loss:\t\t0.144895\n",
            "  validation loss:\t\t0.084571\n",
            "  validation accuracy:\t\t97.47 %\n",
            "Epoch 9 of 50 took 15.865s\n",
            "  training loss:\t\t0.135358\n",
            "  validation loss:\t\t0.079511\n",
            "  validation accuracy:\t\t97.61 %\n",
            "Epoch 10 of 50 took 15.709s\n",
            "  training loss:\t\t0.128017\n",
            "  validation loss:\t\t0.077457\n",
            "  validation accuracy:\t\t97.70 %\n",
            "Epoch 11 of 50 took 15.704s\n",
            "  training loss:\t\t0.121175\n",
            "  validation loss:\t\t0.076339\n",
            "  validation accuracy:\t\t97.80 %\n",
            "Epoch 12 of 50 took 15.531s\n",
            "  training loss:\t\t0.115570\n",
            "  validation loss:\t\t0.071969\n",
            "  validation accuracy:\t\t97.87 %\n",
            "Epoch 13 of 50 took 15.498s\n",
            "  training loss:\t\t0.107985\n",
            "  validation loss:\t\t0.067817\n",
            "  validation accuracy:\t\t97.95 %\n",
            "Epoch 14 of 50 took 15.359s\n",
            "  training loss:\t\t0.106552\n",
            "  validation loss:\t\t0.067544\n",
            "  validation accuracy:\t\t97.98 %\n",
            "Epoch 15 of 50 took 15.448s\n",
            "  training loss:\t\t0.099363\n",
            "  validation loss:\t\t0.068167\n",
            "  validation accuracy:\t\t97.97 %\n",
            "Epoch 16 of 50 took 15.515s\n",
            "  training loss:\t\t0.095607\n",
            "  validation loss:\t\t0.066031\n",
            "  validation accuracy:\t\t98.11 %\n",
            "Epoch 17 of 50 took 15.466s\n",
            "  training loss:\t\t0.093335\n",
            "  validation loss:\t\t0.064558\n",
            "  validation accuracy:\t\t98.10 %\n",
            "Epoch 18 of 50 took 15.291s\n",
            "  training loss:\t\t0.088547\n",
            "  validation loss:\t\t0.065624\n",
            "  validation accuracy:\t\t98.11 %\n",
            "Epoch 19 of 50 took 15.305s\n",
            "  training loss:\t\t0.089428\n",
            "  validation loss:\t\t0.062971\n",
            "  validation accuracy:\t\t98.13 %\n",
            "Epoch 20 of 50 took 15.352s\n",
            "  training loss:\t\t0.083854\n",
            "  validation loss:\t\t0.062187\n",
            "  validation accuracy:\t\t98.31 %\n",
            "Epoch 21 of 50 took 15.473s\n",
            "  training loss:\t\t0.081962\n",
            "  validation loss:\t\t0.061190\n",
            "  validation accuracy:\t\t98.22 %\n",
            "Epoch 22 of 50 took 15.412s\n",
            "  training loss:\t\t0.080620\n",
            "  validation loss:\t\t0.061189\n",
            "  validation accuracy:\t\t98.26 %\n",
            "Epoch 23 of 50 took 15.298s\n",
            "  training loss:\t\t0.079169\n",
            "  validation loss:\t\t0.061737\n",
            "  validation accuracy:\t\t98.18 %\n",
            "Epoch 24 of 50 took 15.391s\n",
            "  training loss:\t\t0.073099\n",
            "  validation loss:\t\t0.061797\n",
            "  validation accuracy:\t\t98.25 %\n",
            "Epoch 25 of 50 took 15.296s\n",
            "  training loss:\t\t0.072259\n",
            "  validation loss:\t\t0.060624\n",
            "  validation accuracy:\t\t98.14 %\n",
            "Epoch 26 of 50 took 15.367s\n",
            "  training loss:\t\t0.072560\n",
            "  validation loss:\t\t0.059133\n",
            "  validation accuracy:\t\t98.25 %\n",
            "Epoch 27 of 50 took 15.653s\n",
            "  training loss:\t\t0.069315\n",
            "  validation loss:\t\t0.059096\n",
            "  validation accuracy:\t\t98.38 %\n",
            "Epoch 28 of 50 took 15.208s\n",
            "  training loss:\t\t0.069318\n",
            "  validation loss:\t\t0.062114\n",
            "  validation accuracy:\t\t98.25 %\n",
            "Epoch 29 of 50 took 15.219s\n",
            "  training loss:\t\t0.067701\n",
            "  validation loss:\t\t0.059034\n",
            "  validation accuracy:\t\t98.35 %\n",
            "Epoch 30 of 50 took 15.233s\n",
            "  training loss:\t\t0.065573\n",
            "  validation loss:\t\t0.058593\n",
            "  validation accuracy:\t\t98.29 %\n",
            "Epoch 31 of 50 took 15.190s\n",
            "  training loss:\t\t0.064051\n",
            "  validation loss:\t\t0.058611\n",
            "  validation accuracy:\t\t98.32 %\n",
            "Epoch 32 of 50 took 15.400s\n",
            "  training loss:\t\t0.061979\n",
            "  validation loss:\t\t0.056519\n",
            "  validation accuracy:\t\t98.37 %\n",
            "Epoch 33 of 50 took 15.249s\n",
            "  training loss:\t\t0.061888\n",
            "  validation loss:\t\t0.059023\n",
            "  validation accuracy:\t\t98.35 %\n",
            "Epoch 34 of 50 took 15.223s\n",
            "  training loss:\t\t0.061334\n",
            "  validation loss:\t\t0.056449\n",
            "  validation accuracy:\t\t98.45 %\n",
            "Epoch 35 of 50 took 15.242s\n",
            "  training loss:\t\t0.056637\n",
            "  validation loss:\t\t0.058139\n",
            "  validation accuracy:\t\t98.36 %\n",
            "Epoch 36 of 50 took 15.216s\n",
            "  training loss:\t\t0.059933\n",
            "  validation loss:\t\t0.055550\n",
            "  validation accuracy:\t\t98.42 %\n",
            "Epoch 37 of 50 took 15.462s\n",
            "  training loss:\t\t0.055721\n",
            "  validation loss:\t\t0.057154\n",
            "  validation accuracy:\t\t98.43 %\n",
            "Epoch 38 of 50 took 15.325s\n",
            "  training loss:\t\t0.053032\n",
            "  validation loss:\t\t0.056274\n",
            "  validation accuracy:\t\t98.39 %\n",
            "Epoch 39 of 50 took 15.263s\n",
            "  training loss:\t\t0.052904\n",
            "  validation loss:\t\t0.056416\n",
            "  validation accuracy:\t\t98.40 %\n",
            "Epoch 40 of 50 took 15.264s\n",
            "  training loss:\t\t0.053272\n",
            "  validation loss:\t\t0.056054\n",
            "  validation accuracy:\t\t98.47 %\n",
            "Epoch 41 of 50 took 15.238s\n",
            "  training loss:\t\t0.052744\n",
            "  validation loss:\t\t0.057370\n",
            "  validation accuracy:\t\t98.42 %\n",
            "Epoch 42 of 50 took 15.450s\n",
            "  training loss:\t\t0.052656\n",
            "  validation loss:\t\t0.057239\n",
            "  validation accuracy:\t\t98.36 %\n",
            "Epoch 43 of 50 took 15.461s\n",
            "  training loss:\t\t0.052066\n",
            "  validation loss:\t\t0.055436\n",
            "  validation accuracy:\t\t98.39 %\n",
            "Epoch 44 of 50 took 15.321s\n",
            "  training loss:\t\t0.049937\n",
            "  validation loss:\t\t0.056430\n",
            "  validation accuracy:\t\t98.45 %\n",
            "Epoch 45 of 50 took 15.322s\n",
            "  training loss:\t\t0.050048\n",
            "  validation loss:\t\t0.056308\n",
            "  validation accuracy:\t\t98.43 %\n",
            "Epoch 46 of 50 took 15.360s\n",
            "  training loss:\t\t0.048783\n",
            "  validation loss:\t\t0.056253\n",
            "  validation accuracy:\t\t98.39 %\n",
            "Epoch 47 of 50 took 15.564s\n",
            "  training loss:\t\t0.050484\n",
            "  validation loss:\t\t0.056588\n",
            "  validation accuracy:\t\t98.52 %\n",
            "Epoch 48 of 50 took 15.549s\n",
            "  training loss:\t\t0.046921\n",
            "  validation loss:\t\t0.055818\n",
            "  validation accuracy:\t\t98.42 %\n",
            "Epoch 49 of 50 took 15.338s\n",
            "  training loss:\t\t0.048409\n",
            "  validation loss:\t\t0.055936\n",
            "  validation accuracy:\t\t98.49 %\n",
            "Epoch 50 of 50 took 15.332s\n",
            "  training loss:\t\t0.045336\n",
            "  validation loss:\t\t0.054479\n",
            "  validation accuracy:\t\t98.50 %\n",
            "Final results:\n",
            "  test loss:\t\t\t0.047123\n",
            "  test accuracy:\t\t98.46 %\n",
            "saved the model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrXiqwHaT5-N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "468ca2e6-cb2d-4653-c50d-5384a33d8bd3"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('model.npz')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d972f302-18ad-44e5-9e7e-7d4ea17f5a88\", \"model.npz\", 10215954)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}